## Question 40: What testing strategies are suitable for LangGraph workflows?

**Difficulty:** hard | **Tags:** testing

### Suitable Testing Strategies for LangGraph Workflows

**LangGraph** enables the construction of complex, stateful, and agentic workflows using language models. Given the dynamic and multi-node nature of these workflows, robust testing is essential to ensure reliability and maintainability. Here are the most suitable testing strategies:

---

#### 1. **Unit Testing of Individual Nodes**
- **Key Concept:** Test each node (function or agent) in isolation, independent of the full workflow.
- **How:** Use standard Python testing frameworks like `unittest` or `pytest` to call node functions directly with controlled input and assert expected output.
- **Example:**
    ```python
    def test_llm_call_1():
        state = {"input": "Tell me a story"}
        result = llm_call_1(state)
        assert "output" in result
    ```
- **Best Practice:** Mock external dependencies (e.g., LLM calls, API requests) to avoid side effects and ensure tests are deterministic.
- **Pitfall:** Not isolating nodes can lead to brittle tests that fail due to unrelated workflow changes.

---

#### 2. **Integration Testing of Workflow Paths**
- **Key Concept:** Test sequences of nodes (subgraphs) or the entire workflow to ensure correct state transitions and data flow.
- **How:** Invoke the compiled workflow with representative input and verify the end-to-end output.
- **Example:**
    ```python
    def test_assistant():
        inputs = {"messages": ["What's the weather like in Paris?"]}
        result = app.invoke(inputs)
        assert "Paris" in result.get("response", "")
    ```
- **Best Practice:** Use a variety of input scenarios, including edge cases, to cover different workflow branches.
- **Pitfall:** Relying only on integration tests can make it hard to pinpoint the source of failures.

---

#### 3. **Mocking and Dependency Injection**
- **Key Concept:** Replace real LLM/tool calls with mocks or stubs during tests to control outputs and simulate errors.
- **How:** Use libraries like `unittest.mock` to patch LLM/tool calls within nodes.
- **Example:**
    ```python
    from unittest.mock import patch

    @patch('module.llm.invoke')
    def test_node_with_mocked_llm(mock_invoke):
        mock_invoke.return_value.content = "Mocked response"
        # ... test logic ...
    ```
- **Best Practice:** Mock at the boundary of the node, not deep internals, to keep tests maintainable.

---

#### 4. **Automated Evaluation with LangSmith**
- **Key Concept:** Use LangSmithâ€™s evaluation tools to run experiments and compare workflow outputs against expected results or metrics.
- **How:** Define evaluators (e.g., correctness, latency) and run them on workflow outputs.
- **Reference:** [LangSmith Evaluate Graph Docs](https://docs.langchain.com/langsmith/evaluate-graph)
- **Best Practice:** Automate regression tests to catch unintended changes in workflow behavior.

---

#### 5. **End-to-End (E2E) Testing**
- **Key Concept:** Simulate real user interactions with the workflow, including all external integrations.
- **How:** Run the workflow in a staging environment with real or sandboxed APIs and LLMs.
- **Best Practice:** Use E2E tests sparingly due to cost and flakiness, focusing on critical user journeys.

---

### Real-World Example

A typical test setup for a LangGraph-powered assistant might include:
- **Unit tests** for each node (e.g., weather lookup, email writer).
- **Integration tests** for each workflow branch (e.g., weather query, email query, general chat).
- **Mocked LLM/tool calls** to ensure tests are fast and reliable.
- **LangSmith evaluations** to track performance and correctness over time.

---

### Summary Table

| Strategy                | Scope         | Tools         | Focus                |
|-------------------------|--------------|---------------|----------------------|
| Unit Testing            | Node         | pytest, unittest | Logic correctness    |
| Integration Testing     | Subgraph/Full| pytest, unittest | Data flow, transitions|
| Mocking                 | Node/Workflow| unittest.mock | Isolation, error simulation|
| Automated Evaluation    | Workflow     | LangSmith     | Output quality, regression|
| End-to-End Testing      | Full         | Custom scripts| Real-world behavior  |

---

### References
- [Best Practices for Testing LangGraph Nodes Separately (LangChain Forum)](https://forum.langchain.com/t/best-practices-for-testing-langgraph-nodes-separately/1396)
- [LangSmith Evaluate Graph Documentation](https://docs.langchain.com/langsmith/evaluate-graph)
- [LangGraph Guide (Medium)](https://medium.com/@vinodkrane/langgraph-a-simple-guide-to-building-smart-ai-workflows-ebc632109428)

---

**In summary:** Combine unit, integration, and E2E tests, use mocking for isolation, and leverage LangSmith for automated evaluation to ensure robust LangGraph workflows. Avoid over-reliance on E2E tests and always isolate node logic for maintainability.

---

